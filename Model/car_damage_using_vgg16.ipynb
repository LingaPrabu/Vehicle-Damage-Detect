{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(rescale=1./255,\n",
    "                                 shear_range=0.1,\n",
    "                                 zoom_range=0.1,\n",
    "                                 horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 979 images belonging to 3 classes.\n",
      "Found 171 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "trainPath = \"../Dataset/Car damage/body/training\"\n",
    "testPath = \"../Dataset/Car damage/body/validation\"\n",
    "training_set=train_datagen.flow_from_directory(trainPath,\n",
    "                                               target_size=(224,224),\n",
    "                                               batch_size=10,\n",
    "                                               class_mode='categorical')\n",
    "test_set=test_datagen.flow_from_directory(testPath,\n",
    "                                               target_size=(224,224),\n",
    "                                               batch_size=10,\n",
    "                                               class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 979 images belonging to 3 classes.\n",
      "Found 171 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "trainPath1 = \"../Dataset/Car damage/level/training\"\n",
    "testPath1 = \"../Dataset/Car damage/level/validation\"\n",
    "training_set1=train_datagen.flow_from_directory(trainPath1,\n",
    "                                               target_size=(224,224),\n",
    "                                               batch_size=10,\n",
    "                                               class_mode='categorical')\n",
    "test_set1=test_datagen.flow_from_directory(testPath1,\n",
    "                                               target_size=(224,224),\n",
    "                                               batch_size=10,\n",
    "                                               class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg=VGG16(weights='imagenet',include_top=False,input_tensor=Input(shape=(224,224,3)))\n",
    "vgg1=VGG16(weights='imagenet',include_top=False,input_tensor=Input(shape=(224,224,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    layer.trainable=False\n",
    "x=Flatten()(vgg.output)\n",
    "for layer in vgg1.layers:\n",
    "    layer.trainable=False\n",
    "y=Flatten()(vgg1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=Dense(3,activation='softmax')(x)\n",
    "prediction1=Dense(3,activation='softmax')(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=vgg.input,outputs=prediction)\n",
    "model1=Model(inputs=vgg1.input,outputs=prediction1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELCOT\\AppData\\Local\\Temp\\ipykernel_5900\\3143213416.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  r=model.fit_generator(training_set,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "97/97 [==============================] - 1677s 17s/step - loss: 1.1566 - acc: 0.5686 - val_loss: 0.8822 - val_acc: 0.6353\n",
      "Epoch 2/25\n",
      "97/97 [==============================] - 1653s 17s/step - loss: 0.6687 - acc: 0.7544 - val_loss: 0.8208 - val_acc: 0.6824\n",
      "Epoch 3/25\n",
      "97/97 [==============================] - 1638s 17s/step - loss: 0.4845 - acc: 0.8204 - val_loss: 1.0894 - val_acc: 0.6353\n",
      "Epoch 4/25\n",
      "97/97 [==============================] - 1635s 17s/step - loss: 0.3835 - acc: 0.8586 - val_loss: 1.1416 - val_acc: 0.6529\n",
      "Epoch 5/25\n",
      "97/97 [==============================] - 1636s 17s/step - loss: 0.2441 - acc: 0.9174 - val_loss: 1.1818 - val_acc: 0.6471\n",
      "Epoch 6/25\n",
      "97/97 [==============================] - 1634s 17s/step - loss: 0.2237 - acc: 0.9216 - val_loss: 1.0819 - val_acc: 0.6471\n",
      "Epoch 7/25\n",
      "97/97 [==============================] - 1639s 17s/step - loss: 0.2243 - acc: 0.9278 - val_loss: 1.1719 - val_acc: 0.6588\n",
      "Epoch 8/25\n",
      "97/97 [==============================] - 1636s 17s/step - loss: 0.2068 - acc: 0.9309 - val_loss: 1.0484 - val_acc: 0.6588\n",
      "Epoch 9/25\n",
      "97/97 [==============================] - 1640s 17s/step - loss: 0.1439 - acc: 0.9577 - val_loss: 1.1413 - val_acc: 0.6765\n",
      "Epoch 10/25\n",
      "97/97 [==============================] - 1638s 17s/step - loss: 0.1329 - acc: 0.9546 - val_loss: 1.0880 - val_acc: 0.7176\n",
      "Epoch 11/25\n",
      "97/97 [==============================] - 1635s 17s/step - loss: 0.1035 - acc: 0.9711 - val_loss: 1.1468 - val_acc: 0.6706\n",
      "Epoch 12/25\n",
      "97/97 [==============================] - 1645s 17s/step - loss: 0.1052 - acc: 0.9742 - val_loss: 1.1056 - val_acc: 0.7059\n",
      "Epoch 13/25\n",
      "97/97 [==============================] - 1638s 17s/step - loss: 0.0805 - acc: 0.9783 - val_loss: 1.0346 - val_acc: 0.6882\n",
      "Epoch 14/25\n",
      "97/97 [==============================] - 1605s 17s/step - loss: 0.0882 - acc: 0.9721 - val_loss: 1.1469 - val_acc: 0.6882\n",
      "Epoch 15/25\n",
      "97/97 [==============================] - 1597s 16s/step - loss: 0.0625 - acc: 0.9876 - val_loss: 1.2008 - val_acc: 0.6706\n",
      "Epoch 16/25\n",
      "97/97 [==============================] - 1595s 16s/step - loss: 0.0895 - acc: 0.9732 - val_loss: 1.6356 - val_acc: 0.6294\n",
      "Epoch 17/25\n",
      "97/97 [==============================] - 1592s 16s/step - loss: 0.0752 - acc: 0.9701 - val_loss: 1.3049 - val_acc: 0.6588\n",
      "Epoch 18/25\n",
      "97/97 [==============================] - 1596s 16s/step - loss: 0.0628 - acc: 0.9886 - val_loss: 1.2008 - val_acc: 0.7059\n",
      "Epoch 19/25\n",
      "97/97 [==============================] - 1588s 16s/step - loss: 0.0639 - acc: 0.9794 - val_loss: 1.2366 - val_acc: 0.7000\n",
      "Epoch 20/25\n",
      "97/97 [==============================] - 1590s 16s/step - loss: 0.0679 - acc: 0.9845 - val_loss: 1.4952 - val_acc: 0.6529\n",
      "Epoch 21/25\n",
      "97/97 [==============================] - 1593s 16s/step - loss: 0.1090 - acc: 0.9701 - val_loss: 1.5410 - val_acc: 0.6529\n",
      "Epoch 22/25\n",
      "97/97 [==============================] - 1569s 16s/step - loss: 0.0510 - acc: 0.9897 - val_loss: 1.3305 - val_acc: 0.6765\n",
      "Epoch 23/25\n",
      "97/97 [==============================] - 1575s 16s/step - loss: 0.0509 - acc: 0.9845 - val_loss: 1.3369 - val_acc: 0.6765\n",
      "Epoch 24/25\n",
      "97/97 [==============================] - 1577s 16s/step - loss: 0.0535 - acc: 0.9866 - val_loss: 1.4180 - val_acc: 0.6412\n",
      "Epoch 25/25\n",
      "97/97 [==============================] - 1597s 16s/step - loss: 0.0686 - acc: 0.9835 - val_loss: 1.4913 - val_acc: 0.6824\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "r=model.fit_generator(training_set,\n",
    "                      validation_data=test_set,\n",
    "                      epochs=25,\n",
    "                      steps_per_epoch=979//10,\n",
    "                      validation_steps=171//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('body.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import  load_model\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "body_model=load_model('body.h5')\n",
    "def detect(frame):\n",
    "    img=cv2.resize(frame,(224,224))\n",
    "    img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "    if(np.max(img)>1):\n",
    "        img=img/255.0\n",
    "    img=np.array([img])\n",
    "    prediction =body_model.predict(img)\n",
    "    print(prediction)\n",
    "    label=[\"front\",\"rear\",\"side\"]\n",
    "    preds=label[np.argmax(prediction)]\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[[0.9522021  0.04156308 0.00623486]]\n",
      "front\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data=\"../accident.jpeg\"\n",
    "image=cv2.imread(data)\n",
    "print(detect(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ELCOT\\AppData\\Local\\Temp\\ipykernel_5900\\1933596904.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  r1= model1.fit_generator(training_set1,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "97/97 [==============================] - 1718s 17s/step - loss: 1.1678 - acc: 0.5593 - val_loss: 0.9057 - val_acc: 0.6118\n",
      "Epoch 2/25\n",
      "97/97 [==============================] - 1664s 17s/step - loss: 0.7936 - acc: 0.7038 - val_loss: 1.2184 - val_acc: 0.5588\n",
      "Epoch 3/25\n",
      "97/97 [==============================] - 1647s 17s/step - loss: 0.5799 - acc: 0.7761 - val_loss: 1.2300 - val_acc: 0.5765\n",
      "Epoch 4/25\n",
      "97/97 [==============================] - 1637s 17s/step - loss: 0.3953 - acc: 0.8504 - val_loss: 1.5192 - val_acc: 0.5412\n",
      "Epoch 5/25\n",
      "97/97 [==============================] - 1631s 17s/step - loss: 0.3721 - acc: 0.8555 - val_loss: 1.5730 - val_acc: 0.5235\n",
      "Epoch 6/25\n",
      "97/97 [==============================] - 1618s 17s/step - loss: 0.2836 - acc: 0.8947 - val_loss: 1.1682 - val_acc: 0.6471\n",
      "Epoch 7/25\n",
      "97/97 [==============================] - 1629s 17s/step - loss: 0.2330 - acc: 0.9143 - val_loss: 1.1455 - val_acc: 0.6235\n",
      "Epoch 8/25\n",
      "97/97 [==============================] - 1637s 17s/step - loss: 0.1548 - acc: 0.9556 - val_loss: 1.4656 - val_acc: 0.5529\n",
      "Epoch 9/25\n",
      "97/97 [==============================] - 1644s 17s/step - loss: 0.1395 - acc: 0.9515 - val_loss: 1.0740 - val_acc: 0.6353\n",
      "Epoch 10/25\n",
      "97/97 [==============================] - 1625s 17s/step - loss: 0.1385 - acc: 0.9556 - val_loss: 1.2268 - val_acc: 0.6765\n",
      "Epoch 11/25\n",
      "97/97 [==============================] - 1618s 17s/step - loss: 0.1501 - acc: 0.9432 - val_loss: 1.4106 - val_acc: 0.6118\n",
      "Epoch 12/25\n",
      "97/97 [==============================] - 1616s 17s/step - loss: 0.0863 - acc: 0.9773 - val_loss: 1.2365 - val_acc: 0.6471\n",
      "Epoch 13/25\n",
      "97/97 [==============================] - 1619s 17s/step - loss: 0.0642 - acc: 0.9835 - val_loss: 1.3294 - val_acc: 0.6412\n",
      "Epoch 14/25\n",
      "97/97 [==============================] - 1605s 17s/step - loss: 0.0549 - acc: 0.9845 - val_loss: 1.3415 - val_acc: 0.6235\n",
      "Epoch 15/25\n",
      "97/97 [==============================] - 1594s 16s/step - loss: 0.0536 - acc: 0.9866 - val_loss: 1.3020 - val_acc: 0.6471\n",
      "Epoch 16/25\n",
      "97/97 [==============================] - 1591s 16s/step - loss: 0.0385 - acc: 0.9948 - val_loss: 1.2475 - val_acc: 0.6353\n",
      "Epoch 17/25\n",
      "97/97 [==============================] - 1620s 17s/step - loss: 0.0482 - acc: 0.9876 - val_loss: 1.4101 - val_acc: 0.6235\n",
      "Epoch 18/25\n",
      "97/97 [==============================] - 1617s 17s/step - loss: 0.0525 - acc: 0.9876 - val_loss: 1.3057 - val_acc: 0.6353\n",
      "Epoch 19/25\n",
      "97/97 [==============================] - 1621s 17s/step - loss: 0.0291 - acc: 0.9990 - val_loss: 1.3371 - val_acc: 0.6647\n",
      "Epoch 20/25\n",
      "97/97 [==============================] - 1613s 17s/step - loss: 0.0245 - acc: 0.9959 - val_loss: 1.3378 - val_acc: 0.6471\n",
      "Epoch 21/25\n",
      "97/97 [==============================] - 1595s 16s/step - loss: 0.0285 - acc: 0.9948 - val_loss: 1.3354 - val_acc: 0.6529\n",
      "Epoch 22/25\n",
      "97/97 [==============================] - 1593s 16s/step - loss: 0.0265 - acc: 0.9969 - val_loss: 1.4192 - val_acc: 0.6353\n",
      "Epoch 23/25\n",
      "97/97 [==============================] - 1610s 17s/step - loss: 0.0164 - acc: 0.9990 - val_loss: 1.4065 - val_acc: 0.6647\n",
      "Epoch 24/25\n",
      "97/97 [==============================] - 1620s 17s/step - loss: 0.0228 - acc: 0.9969 - val_loss: 1.4076 - val_acc: 0.6353\n",
      "Epoch 25/25\n",
      "97/97 [==============================] - 1619s 17s/step - loss: 0.0189 - acc: 0.9969 - val_loss: 1.3946 - val_acc: 0.6353\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "r1= model1.fit_generator(training_set1,\n",
    "                        validation_data=test_set1,\n",
    "                        epochs=25,\n",
    "                        steps_per_epoch=979//10,\n",
    "                        validation_steps=171//10)\n",
    "model1.save('level.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_model = load_model('level.h5')\n",
    "\n",
    "\n",
    "def detect1(frame):\n",
    "    img = cv2.resize(frame, (224, 224))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if(np.max(img) > 1):\n",
    "        img = img/255.0\n",
    "    img = np.array([img])\n",
    "    prediction = level_model.predict(img)\n",
    "    print(prediction)\n",
    "    label = [\"minor\", \"moderate\", \"severe\"]\n",
    "    preds = label[np.argmax(prediction)]\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "[[6.039188e-07 5.539544e-02 9.446039e-01]]\n",
      "severe\n"
     ]
    }
   ],
   "source": [
    "data = \"../0111.JPEG\"\n",
    "image = cv2.imread(data)\n",
    "print(detect1(image))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fdca3382253761908f5ccc1531889bf4867ea4dc34243de6c1b27b50c87f32d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
